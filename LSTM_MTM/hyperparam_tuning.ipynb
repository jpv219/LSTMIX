{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LSTM hyperparameter tuning<br>\n", "# Author: Juan Pablo Valdes<br>\n", "# Code adapted from Fuyue Liang LSTM for stirred vessels<br>\n", "# First commit: Oct, 2023<br>\n", "# Department of Chemical Engineering, Imperial College London<br>\n", "#######################################################################################################################################################<br>\n", "#######################################################################################################################################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import modeltrain_LSTM as trn\n", "from modeltrain_LSTM import LSTM_S2S, LSTM_DMS\n", "from tools_modeltraining import custom_loss\n", "from torch.optim.lr_scheduler import ReduceLROnPlateau\n", "import torch\n", "import torch.optim as optim\n", "import torch.utils.data as data\n", "import pickle\n", "import psutil\n", "import shutil\n", "import os\n", "from functools import partial\n", "import sys\n", "from contextlib import redirect_stdout"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import ray\n", "from ray import tune\n", "from ray.tune.schedulers import ASHAScheduler\n", "import ray.cloudpickle as raypickle"]}, {"cell_type": "markdown", "metadata": {}, "source": [" Env. variables ##"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fig_savepath = '/Users/mfgmember/Documents/Juan_Static_Mixer/ML/LSTM_SMX/LSTM_MTM/figs/'<br>\n", "input_savepath = '/Users/mfgmember/Documents/Juan_Static_Mixer/ML/LSTM_SMX/LSTM_MTM/input_data/'<br>\n", "trainedmod_savepath = '/Users/mfgmember/Documents/Juan_Static_Mixer/ML/LSTM_SMX/LSTM_MTM/trained_models/'<br>\n", "tuning_savepath = '/Users/mfgmember/Documents/Juan_Static_Mixer/ML/LSTM_SMX/LSTM_MTM/tuning'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig_savepath = '/home/fl18/Desktop/automatework/ML_casestudy/LSTM_SMX/LSTM_MTM/figs/'\n", "input_savepath = '/home/fl18/Desktop/automatework/ML_casestudy/LSTM_SMX/LSTM_MTM/input_data/'\n", "trainedmod_savepath = '/home/fl18/Desktop/automatework/ML_casestudy/LSTM_SMX/LSTM_MTM/trained_svmodels/'\n", "tuningmod_savepath = '/media/fl18/Elements/Hypertuning/'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["######################################### METHODS ###########################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_tune(config, model_choice, init, X_tens, y_tens, best_chkpt_path, tuning):\n", "    '''\n", "    init: Initialization (non-tunable) parameters for LSTM class\n", "    config: receives the hyperparameters we would like to train with;\n", "    '''     \n", "    ## Dataloader and loss fun\n", "    loss_fn = custom_loss(config['penalty_weight'])\n", "    \n", "    trainloader = data.DataLoader(data.TensorDataset(X_tens[0], y_tens[0]), \n", "                                  shuffle=True, batch_size=config['batch_size'])\n", "    valloader = data.DataLoader(data.TensorDataset(X_tens[1], y_tens[1]), \n", "                                shuffle=True, batch_size=config['batch_size'])\n", "    \n", "    ## Calling model class instance and training function\n", "    if model_choice == \"DMS\":\n", "        model = LSTM_DMS(init[\"input_size\"],config['hidden_size'],\n", "                         init[\"output_size\"],init[\"pred_steps\"],\n", "                         config[\"l1_lambda\"], config[\"l2_lambda\"])\n", "        \n", "        optimizer = optim.Adam(model.parameters(), lr = config[\"learning_rate\"])\n", "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n", "        if not tuning:\n", "            with open(os.path.join(best_chkpt_path, 'chk_dict.pkl'),'rb') as fp:\n", "                loaded_checkpoint_state = raypickle.load(fp)\n", "                \n", "                model.load_state_dict(loaded_checkpoint_state['model_state_dict'])\n", "                optimizer.load_state_dict(loaded_checkpoint_state['optimizer_state_dict'])\n\n", "        ## Calling training function\n", "        trn.train_DMS(model, optimizer, loss_fn, trainloader, valloader, scheduler, \n", "            init[\"num_epochs\"], init[\"check_epochs\"], X_tens[0], y_tens[0], X_tens[1], \n", "            y_tens[1],saveas='DMS_out',batch_loss=config[\"batch_loss\"],tuning=tuning)\n", "        \n", "    elif model_choice == 'S2S':\n", "        model = LSTM_S2S(init[\"input_size\"],config['hidden_size'],\n", "                         init[\"output_size\"],init[\"pred_steps\"],\n", "                         config[\"l1_lambda\"], config[\"l2_lambda\"])\n", "        \n", "        optimizer = optim.Adam(model.parameters(), lr = config[\"learning_rate\"])\n", "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n", "        if not tuning:\n", "            with open(os.path.join(best_chkpt_path, 'chk_dict.pkl'),'rb') as fp:\n", "                loaded_checkpoint_state = raypickle.load(fp)\n", "                \n", "                model.load_state_dict(loaded_checkpoint_state['model_state_dict'])\n", "                optimizer.load_state_dict(loaded_checkpoint_state['optimizer_state_dict'])\n", "                \n", "        trn.train_S2S(model,optimizer, loss_fn, trainloader, valloader, scheduler, init[\"num_epochs\"], \n", "                  init[\"check_epochs\"],init[\"pred_steps\"],X_tens[0], y_tens[0], X_tens[1], y_tens[1],\n", "                  config[\"tf_ratio\"], config[\"dynamic_tf\"], config[\"training_prediction\"],\n", "                  saveas='S2S_out',batch_loss=config[\"batch_loss\"],tuning=tuning)\n", "    else:\n", "        raise ValueError('Model selected is not configured/does not exist. Double check input.')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_data(model_choice):\n", "    ##### LOADING DATA ######\n\n", "    # Initializing loading containers\n", "    set_labels = [\"train\", \"val\", \"test\"]\n", "    # Will contain training and validation sets in positions 0,1.\n", "    windowed_in_tens = []\n", "    windowed_out_tens = []\n", "    in_casebatch = []\n", "    out_casebatch = []\n", "    test_array = []\n", "    testset_labels = []\n\n", "    ## Loading test_numpy array\n", "    npfile = os.path.join(trainedmod_savepath,f'data_sets_{model_choice}', f'{set_labels[-1]}_pkg.pkl')\n", "    with open(npfile, 'rb') as file:\n", "        test_pkg = pickle.load(file)\n", "    test_array.append(test_pkg[f\"{set_labels[-1]}_arr\"])\n", "    testset_labels.append(test_pkg[\"splitset_labels\"])\n\n", "    ## Loading training and validation windowed tensors\n", "    for setlbl in set_labels:\n", "        in_ptfile = os.path.join(trainedmod_savepath,f'data_sets_{model_choice}', f'X_{setlbl}.pt')\n", "        out_ptfile = os.path.join(trainedmod_savepath,f'data_sets_{model_choice}', f'y_{setlbl}.pt')\n", "        if os.path.exists(in_ptfile) and os.path.exists(out_ptfile):\n\n", "            # X_tensors\n", "            in_savetens = torch.load(in_ptfile)\n", "            windowed_in_tens.append(in_savetens[\"windowed_data\"].to(torch.float32))\n", "            in_casebatch.append(in_savetens[f\"{setlbl}_casebatch\"])\n\n", "            # y_tensors\n", "            out_savetens = torch.load(out_ptfile)\n", "            windowed_out_tens.append(out_savetens[\"windowed_data\"].to(torch.float32))\n", "            out_casebatch.append(out_savetens[f\"{setlbl}_casebatch\"])\n", "    \n", "    return windowed_in_tens, windowed_out_tens, in_casebatch, out_casebatch, test_array, testset_labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_tuning(config, model_choice, init, X_tens, y_tens, scheduler, \n", "               num_samples, log_file_path, best_chkpt_path, tuning):\n", "    \n", "    with open(log_file_path, 'w',encoding='utf-8', errors='ignore') as f, redirect_stdout(f):\n", "        try:\n", "            tuner = tune.run(\n", "                partial(train_tune,\n", "                        model_choice=model_choice,\n", "                        init=init,\n", "                        X_tens=X_tens,\n", "                        y_tens=y_tens,\n", "                        best_chkpt_path=best_chkpt_path,\n", "                        tuning=tuning),\n", "                config=config,\n", "                num_samples=num_samples,\n", "                scheduler=scheduler,\n", "                local_dir=os.path.join(tuningmod_savepath, model_choice)\n", "            )\n", "            return tuner\n", "        \n", "        finally:\n", "            sys.stdout.flush()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def further_train(model_choice, init_training, X_tens, y_tens, best_trial,best_chkpt):\n", "    \n", "    config_training = best_trial.config\n", "    best_chkpt_path = best_chkpt.path\n", "    \n", "    ## save hyperparameters used forfurther  model trained for later plotting and rollout prediction\n", "    hyperparams = {\n", "        \"input_size\": init_training['input_size'],\n", "        \"hidden_size\": config_training['hidden_size'],\n", "        \"output_size\": init_training['output_size'],\n", "        \"pred_steps\": init_training['pred_steps'],\n", "        \"batch_size\": config_training['batch_size'],\n", "        \"learning_rate\": config_training['learning_rate'],\n", "        \"num_epochs\": init_training['num_epochs'],\n", "        \"check_epochs\": init_training['check_epochs'],\n", "        \"steps_in\": init_training['steps_in'],\n", "        \"steps_out\": init_training['steps_out'],\n", "        \"tf_ratio\": config_training['tf_ratio'],\n", "        \"dynamic_tf\": config_training['dynamic_tf']\n", "    }\n", "    with open(os.path.join(trainedmod_savepath,f'hyperparams_{model_choice}.txt'), \"w\") as file:\n", "        for key, value in hyperparams.items():\n", "            file.write(f\"{key}: {value}\\n\")\n\n", "    ## Set to train further mode\n", "    train_tune(config_training, model_choice, init_training, X_tens, y_tens, best_chkpt_path, tuning=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["######################################### MAIN ###########################################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    model_choice = input('Select a LSTM model to tune (DMS, S2S): ')\n", "    \n", "    ## Load windowed tensors for training and val\n", "    X_tens, y_tens, _, _ ,_, _ = load_data(model_choice)\n\n", "    # limit the number of CPU cores used for the whole tuning process\n", "    percent_cpu_to_occupy = 0.3\n", "    total_cpus = psutil.cpu_count(logical=False)\n", "    num_cpus_to_allocate = int(total_cpus * percent_cpu_to_occupy)\n", "    search_spaces = {\n", "        'DMS': {\n", "            'hidden_size': tune.choice([2 ** i for i in range(6, 9)]),\n", "            'learning_rate': tune.choice([0.005]),\n", "            'batch_size': tune.choice(range(10, 40, 5)),\n", "            'training_prediction': tune.choice(['none']),\n", "            'tf_ratio': tune.choice([0]),\n", "            'dynamic_tf': tune.choice(['False']),\n", "            'l1_lambda': tune.choice([0.00001, 0.0001, 0.001]),\n", "            'l2_lambda': tune.choice([0.00001, 0.0001, 0.001]),\n", "            'batch_loss': tune.choice(['True', 'False']),\n", "            'penalty_weight': tune.choice([1, 10])\n", "        },\n", "        'S2S': {\n", "            'hidden_size': tune.choice([2 ** i for i in range(6, 9)]),\n", "            'learning_rate': tune.choice([0.005]),\n", "            'batch_size': tune.choice(range(10, 40, 10)),\n", "            'training_prediction': tune.choice(['teacher_forcing', 'mixed']),\n", "            'tf_ratio': tune.choice([0.02, 0.1, 0.2]),\n", "            'dynamic_tf': tune.choice(['True']),\n", "            'l1_lambda': tune.choice([0.00001, 0.0001, 0.001]),\n", "            'l2_lambda': tune.choice([0.00001, 0.0001, 0.001]),\n", "            'batch_loss': tune.choice(['False']),\n", "            'penalty_weight': tune.choice([1, 10])\n", "        }\n", "    }\n", "    \n", "    search_space = search_spaces[model_choice]\n", "    \n", "    # Set constant parameters to intialize the LSTM\n", "    init = {\n", "        \"input_size\": X_tens[0].shape[-1],\n", "        \"output_size\": y_tens[0].shape[-1],\n", "        \"pred_steps\": 50,\n", "        \"num_epochs\": 100,\n", "        \"check_epochs\": 20\n", "    }\n\n", "    # Configure and run RAY TUNING \n", "    scheduler = ASHAScheduler(\n", "    metric='val_loss',\n", "    mode='min',\n", "    max_t= init[\"num_epochs\"],\n", "    grace_period=40, # save period without early stopping\n", "    reduction_factor=2,\n", "    )\n", "    ray.shutdown()\n", "    ray.init(num_cpus=num_cpus_to_allocate)\n", "    num_samples = 400\n", "    log_file_path = os.path.join(tuningmod_savepath,model_choice,f'logs/{model_choice}_tune_out.log')\n\n", "    # Run the experiment\n", "    tuner = run_tuning(search_space, model_choice, init, X_tens, \n", "                       y_tens,scheduler, num_samples, log_file_path, best_chkpt_path='', tuning=True)\n", "    \n", "    # Extract results from tuning process\n", "    best_trial = tuner.get_best_trial('val_loss', 'min', 'last')\n", "    best_chkpoint = tuner.get_best_checkpoint(best_trial,'val_loss','min')\n", "    ray.shutdown()\n", "    print(f'Finished tuning hyperparameters with {num_samples} samples')\n", "    print(f'Best trial id: {best_trial.trial_id}')\n", "    print(f'Best trial config: {best_trial.config}')\n\n", "    # Saving best model and config to external path\n", "    best_model_path = os.path.join(tuningmod_savepath,f'best_models/{model_choice}')\n", "    shutil.copy(f'{best_chkpoint.path}/chk_dict.pkl',best_model_path)\n", "    with open(f'{best_model_path}/config_{model_choice}.pkl', 'wb') as pickle_file:\n", "        pickle.dump(best_trial.config, pickle_file)\n", "    print('Model state and config settings copied to best_model folder')\n\n", "    #### FURTHER TRAINING WITH TUNED MODEL ###\n", "    train_further = input('Train best tuned trial further? (y/n): ')\n", "    if train_further.lower() == 'y':\n", "    \n", "        ## Setting new init and config parameters for further training of best trial tuned\n", "        init_training = {\n", "            \"input_size\": X_tens[0].shape[-1],\n", "            \"output_size\": y_tens[0].shape[-1],\n", "            \"pred_steps\": 50,\n", "            \"num_epochs\": 2000,\n", "            \"check_epochs\": 100,\n", "            \"steps_in\": 50,\n", "            \"steps_out\": 50\n", "    }\n", "        \n", "        further_train(model_choice,init_training,X_tens,y_tens,best_trial,best_chkpoint)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}