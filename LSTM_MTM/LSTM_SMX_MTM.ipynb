{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM-RNN MTM model to predict temporal evolution of: Number of drops (Nd), Interfacial Area (IA), and size distribution (DSD)\n",
    "### Code adapted for L-L SMX static mixer\n",
    "### Author: Juan Pablo Valdes\n",
    "### First commit: Aug, 2023\n",
    "### Code adapted from: LSTM_MTM by Fuyue Liang, 2023 for stirred vessels\n",
    "### Department of Chemical Engineering, Imperial College London"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LSTM timeseries prediction for SMX dispersion performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##All imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Load_Clean_DF\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading and cleaning up raw data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_3D = ['b03','b06','b09','bi001','bi01','bi1','da001','da01','da1']\n",
    "\n",
    "cases_MD = ['b06pm','b09pm','bi001pm','bi01pm','da01pm']\n",
    "\n",
    "cases_clean = ['3drop', 'coarsepm']\n",
    "\n",
    "Allcases = ['b03','b06','bi001','bi01','da01','da1','b06pm','b09pm','bi001pm',\n",
    "'bi1','bi01pm','3drop',\n",
    "'b09','da01pm','da001', 'coarsepm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Importing rawdata from pre-processed csv files into dataframes</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_rawdata(case):\n",
    "    if case == '3drop' or case == 'coarsepm':\n",
    "        # If true, extract only volume array\n",
    "        df_Vol = Load_Clean_DF.extract_Vol(case)\n",
    "    else:\n",
    "        # If false, extract volume and concentration arrays\n",
    "        df_Vol = Load_Clean_DF.extract_GVol(case)\n",
    "    \n",
    "    # Extract number of drops (Nd) and interfacial area (IntA)\n",
    "    Nd = Load_Clean_DF.extract_Nd(case)\n",
    "    IntA = Load_Clean_DF.extract_IA(case)\n",
    "\n",
    "    return df_Vol, Nd, IntA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Storing all data into a single dictionary to be used later by the LSTM</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dicts to hold all data extracted from HR sims, before and after post-process\n",
    "pre_dict = {}\n",
    "post_dict = {}\n",
    "\n",
    "# Loop through all cases\n",
    "for case in Allcases:\n",
    "    # Extract raw data\n",
    "    df_Vol, Nd, IntA = import_rawdata(case)\n",
    "    \n",
    "    time = Nd['Time']\n",
    "    n_drops = Nd['Ndrops']\n",
    "    IA = IntA['IA']\n",
    "    DSD = df_Vol['Volume']\n",
    "    \n",
    "    # Determine if case needs surf. conc. or clean\n",
    "    if case == '3drop' or case == 'coarsepm':\n",
    "        G = []  # If true, set G as an empty list\n",
    "    else:\n",
    "        G = df_Vol['Gammatilde']  # If false, extract G data\n",
    "    \n",
    "    pre_dict[case] = {'Time': time, 'Nd': n_drops, 'IA': IA, 'Vol': DSD, 'G': G}\n",
    "    \n",
    "    # Initialize an empty post-process dict per case\n",
    "    post_dict[case] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting first with Nd and IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Scale and normalise data to be handled for LSTM</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MinMaxScaler from sklearn for scaling and normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# List of columns to be normalized\n",
    "norm_columns = ['Nd', 'IA']\n",
    "\n",
    "for case in Allcases:\n",
    "    norm_data_case = pre_dict[case]\n",
    "\n",
    "    # Loop through each column to be normalized\n",
    "    for column in norm_columns:\n",
    "        norm_data = norm_data_case[column].values.astype('float64')\n",
    "        \n",
    "        # Reshape the data to be compatible with the scaler\n",
    "        norm_data = norm_data.reshape(-1, 1)\n",
    "        \n",
    "        # Create a MinMaxScaler and fit it to the data\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler = scaler.fit(norm_data)\n",
    "        \n",
    "        # Transform and store the normalized data in the post_data dict.\n",
    "        post_dict[case][column] = scaler.transform(norm_data).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>Stacking all data into a numpy array and reshaping it as : (timesteps, num_cases, num_features)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(time_step, num_case, num_feature)= (105, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "### All cases must have the same number of data points for them to be used in LSTM\n",
    "\n",
    "min_length = min(len(data['IA']) for data in post_dict.values())\n",
    "\n",
    "# Iterate through each case\n",
    "for case, features in post_dict.items():\n",
    "    # Extract the 'Nd' and 'IA' data for the current case\n",
    "    # Cases are truncated with the min length in order to be stacked as a nparray\n",
    "    Nd_data = features['Nd'][:min_length]\n",
    "    IA_data = features['IA'][:min_length]\n",
    "\n",
    "    \n",
    "    # Combine 'Nd_data' and 'IA_data' into a single numpy array\n",
    "    combined_data = np.column_stack((Nd_data, IA_data))\n",
    "    \n",
    "    # Append the combined data to the data_list\n",
    "    array.append(combined_data)\n",
    "\n",
    "# Stack the data_list along a new axis to create the final numpy array\n",
    "mydata = np.stack(array, axis=1)\n",
    "print('(time_step, num_case, num_feature)=', mydata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### looping over the number of features (Nd and IA)\n",
    "\n",
    "#Plot setup\n",
    "rc('text', usetex=True)\n",
    "custom_font = {'family': 'serif', 'serif': ['Computer Modern Roman']}\n",
    "rc('font', **custom_font)\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "features = ['Number of drops', 'Interfacial Area']\n",
    "colors = sns.color_palette(\"husl\", len(Allcases))\n",
    "\n",
    "for i in range(mydata.shape[-1]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'{features[i]}')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[i]}')\n",
    "\n",
    "    ### Feature 1 and 2 being Nd and IA, respectively\n",
    "    for case, idx in zip(Allcases, range(len(Allcases))):\n",
    "        plt.plot(mydata[:,idx,i], label = f'{str(case)}',color=colors[idx % len(colors)])\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "def smoothing(data, method, window_size=None, poly_order=None, lowess_frac = None):\n",
    "    '''\n",
    "    Input array : 2D array per feature, with shape (times, cases)\n",
    "    Three methods to smooth:\n",
    "    \n",
    "    'moveavg': requires window_size\n",
    "    'savgol': requires window_size, poly_order\n",
    "    'lowess': requires lowess_frac\n",
    "    '''\n",
    "\n",
    "    ## rolling window averaging method\n",
    "    if method == 'moveavg':\n",
    "        if window_size is None:\n",
    "            raise ValueError('Window size required')\n",
    "        smoothed_data = pd.DataFrame(data).rolling(window_size, axis = 0).mean()\n",
    "        smoothed_data.fillna(pd.DataFrame(data),inplace=True)\n",
    "    ## SavGol filter based on fitting least-squares polynomial to a window of data points\n",
    "    elif method == 'savgol':\n",
    "        if window_size is None or poly_order is None:\n",
    "            raise ValueError('Mising input arguments: Windowsize/polyorder')\n",
    "        smoothed_data = np.apply_along_axis(\n",
    "                    lambda col: savgol_filter(col, window_size, poly_order),\n",
    "                    axis = 0, arr=data)\n",
    "    ## Locally Weighted Scatterplot Smoothing, locally fitting linear regressions\n",
    "    elif method == 'lowess':\n",
    "        if lowess_frac is None:\n",
    "            raise ValueError('Lowess fraction required')\n",
    "        smoothed_data = np.apply_along_axis(\n",
    "                    lambda col: lowess(col,np.arange(len(col)),frac = lowess_frac,return_sorted=False),\n",
    "                    axis = 0, arr = data)\n",
    "    else:\n",
    "        raise ValueError('Unsupported smoothing method')\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "    for case, idx in zip(Allcases, range(len(Allcases))):\n",
    "        ax[0].plot(data[:,idx], label = f'{str(case)}',color=colors[idx % len(colors)])\n",
    "        ax[0].set_title('Nd before')\n",
    "        ax[0].set_xlabel('Time steps')\n",
    "        ax[0].set_ylabel('Scaled Nd')\n",
    "        \n",
    "        ax[1].plot(smoothed_data[:,idx], label = f'{str(case)}',color=colors[idx % len(colors)])\n",
    "        ax[1].set_title('Nd after')\n",
    "        ax[1].set_xlabel('Time steps')\n",
    "        ax[1].set_ylabel('Smoothed Nd')\n",
    "    \n",
    "    fig.suptitle(f'Smoothing method: {method}', fontsize=18)\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mydata[:,:,0] # 0 for drop number and 1 for IA\n",
    "#smoothed_data = smoothing(data,'lowess',lowess_frac=0.06)\n",
    "smoothed_data = smoothing(data,'savgol',window_size=5,poly_order=3)\n",
    "#smoothed_data = smoothing(data,'moveavg',window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Train, validation and test data set splitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split by case\n",
    "def cases_split(df, train_frac, test_frac, cases):\n",
    "    '''\n",
    "    input shape: (times, cases, features)\n",
    "    \n",
    "    return train, val data and cases\n",
    "    '''\n",
    "    train_size = int(df.shape[1]*train_frac)\n",
    "\n",
    "    val_size = int(df.shape[1]*(1-test_frac-train_frac))\n",
    "    \n",
    "    train, val, test = df[:, :train_size, :], df[:, train_size:(train_size+val_size), :], df[:,(train_size+val_size):,:]\n",
    "    print(f'number of train, val and test cases: {train.shape[1]}, {val.shape[1]}, {test.shape[1]}')\n",
    "    \n",
    "    train_cases, val_cases , test_cases = cases[:train_size], cases[train_size:(train_size+val_size)], cases[(train_size+val_size):]\n",
    "    print(f'training cases: {train_cases}, validation cases: {val_cases}, test cases: {test_cases}')\n",
    "        \n",
    "    return train, val, test, train_cases, val_cases, test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.5625\n",
    "val_frac = 0.1875\n",
    "test_frac = 0.25\n",
    "\n",
    "## Replacing noisy Nd data with smoothed data\n",
    "mydata[:,:,0] = smoothed_data\n",
    "\n",
    "f_data = mydata\n",
    "\n",
    "train, val, test, train_cases, val_cases, test_cases = cases_split(f_data, train_frac=train_frac, test_frac = test_frac, cases=Allcases)\n",
    "\n",
    "print(f'train, val, test shapes: {train.shape}, {val.shape}, {test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = sns.color_palette(\"Set1\", len(Allcases))\n",
    "c2 = sns.color_palette(\"Set2\", len(Allcases))\n",
    "c3 = sns.color_palette(\"Set3\", len(Allcases))\n",
    "\n",
    "# Training cases\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(mydata.shape[-1]):\n",
    "    for case, idx in zip(train_cases, range(len(train_cases))):\n",
    "        ax[i].plot(train[:,idx,i],label = f'{str(case)}',color=c1[idx % len(colors)])\n",
    "        ax[i].set_title(f'Training: {features[i]}')\n",
    "        ax[i].set_xlabel('Time steps')\n",
    "        ax[i].set_ylabel(f'Scaled {features[i]}')\n",
    "        ax[i].legend()\n",
    "\n",
    "# Validation cases\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(mydata.shape[-1]):\n",
    "    for case, idx in zip(val_cases, range(len(val_cases))):\n",
    "        ax[i].plot(val[:,idx,i],label = f'{str(case)}',color=c2[idx % len(colors)])\n",
    "        ax[i].set_title(f'Validation: {features[i]}')\n",
    "        ax[i].set_xlabel('Time steps')\n",
    "        ax[i].set_ylabel(f'Scaled {features[i]}')\n",
    "        ax[i].legend()\n",
    "\n",
    "# Test cases\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(mydata.shape[-1]):\n",
    "    for case, idx in zip(test_cases, range(len(test_cases))):\n",
    "        ax[i].plot(test[:,idx,i],label = f'{str(case)}',color=c3[idx % len(colors)])\n",
    "        ax[i].set_title(f'Test: {features[i]}')\n",
    "        ax[i].set_xlabel('Time steps')\n",
    "        ax[i].set_ylabel(f'Scaled {features[i]}')\n",
    "        ax[i].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Sampling prediction dataset (windowing)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def window_data(df, steps_in, stride, steps_out):\n",
    "    '''\n",
    "    \n",
    "    df: with shape (times, cases, features)\n",
    "    stride: the step size between consecutive windows\n",
    "    pred_times:(<window_size) predicted future times from current window\n",
    "    window size: Encompasses both steps_in and steps_out, referring to input seq and prediction seq\n",
    "    \n",
    "    lookback period = window_size - steps_out\n",
    "     \n",
    "    '''\n",
    "    window_size = steps_in + steps_out\n",
    "    casebatch_lens = [] # List to contain the number of rows/windows per case used for input-->prediction based on the steps_in - steps_out parameters\n",
    "                        # Can be calculated as: len(timesteps)-window_size+1\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(df.shape[1]): # looping for each case, df shape of (times, cases, features)\n",
    "        df_case = df[:,i,:] # df per case\n",
    "        for j in range(0, len(df_case)-window_size+1, stride): # Looping over timesteps based on the window size\n",
    "            wd_data = df_case[j:j+window_size] # window with times: steps_in + steps_out\n",
    "            X.append(wd_data[:-steps_out]) #input values, steps_in\n",
    "            y.append(wd_data[-steps_out:]) #training/ prediction values, steps_out\n",
    "        casebatch_lens.append(len(X)) # appending casebatch length per case \n",
    "\n",
    "    ## number of windows/rows with size (steps_in) per case, used to later plot per case\n",
    "    print(casebatch_lens)\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(y), np.array(casebatch_lens)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_in, steps_out = 36, 20\n",
    "wid_size = steps_in + steps_out\n",
    "stride = 1\n",
    "\n",
    "### Creating the tensors (sampling) after splitting train and validation data sets\n",
    "X_train, y_train, train_casebatch = window_data(train, steps_in=steps_in, stride=stride, steps_out=steps_out)\n",
    "X_val, y_val, val_casebatch = window_data(val, steps_in=steps_in, stride=stride, steps_out=steps_out)\n",
    "\n",
    "# Check the shape of the output tensor\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "## output shape should be (windows among all cases: \n",
    "# number of windows/rows per case X number of cases, times per \n",
    "# input/prediction row: steps_in/steps_out, features per case)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM MTM (Many-to-Many):\n",
    "\n",
    "An LSTM unit generates three main outputs:\n",
    "\n",
    "1. <sub>**Cell state**: This could be thought of as long-term memory, representing the aggregate information of all previous LSTM units (timesteps). It is selectively remembered or forgotten at each LSTM unit.</sub>\n",
    "\n",
    "2. <sub>**Hidden state**: This is often thought of as short-term memory. It characterizes the most recent timestep, influenced by the cell state and the input, and used to modify the cell state. Here we can consider 'neurons' as in a regular NN through the hidden size, which determines the size of the matrix of weights used to compute the hidden state. This is independent of the number of features but it modifies the size of the matrix of input weights accordingly. </sub>\n",
    "\n",
    "3. <sub>**Output**: The output is derived directly from the hidden state and represents the final prediction after being properly decoded.</sub>\n",
    "\n",
    "\n",
    "## 1) Direct Multi-step DMS approach\n",
    "\n",
    "- <sub> In this approach, we consider a window formatted as steps_in (input) and steps_out (prediction) and we roll the window forward based on the stride. The number of rolls/rows/windows depends on the number of timesteps and the size of the row selected (steps_in and steps_out). \n",
    "\n",
    "- <sub> The first LSTM unit receives as input a batch containing the first value of each window, depending on the batch size hyperparameter. After calculations are done, the next LSTM unit receives the previous cell state, hidden state and as input the second value of each window in the batch and continues accordingly.\n",
    "\n",
    "- <sub> In this approach, we use the last LSTM unit's output (hidden state) and pass it through a dense/linear layer to make predictions for all future time steps simultaneously. We force this multi output scenario. </sub>\n",
    "\n",
    "<sub>The LSTM output layer produces multiple outputs, allowing it to be trained to directly predict multiple future time points in one pass.</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "### LSTM model class\n",
    "class LSTM_DMS(nn.Module):\n",
    "\n",
    "    ## class constructor\n",
    "    def __init__(self, input_size, hidden_size, output_size, pred_steps,\n",
    "                 l1_lambda=0.0, l2_lambda=0.0):\n",
    "        # calling the constructor of the parent class nn.Module to properly intialize this class\n",
    "        super(LSTM_DMS,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pred_steps = pred_steps # prediction steps = steps_out\n",
    "        self.output_size = output_size\n",
    "        ## LSTM unit instance from parent class\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) \n",
    "        # Linear layer instance from parent class, for multi-step predictions\n",
    "        self.linear = nn.Linear(hidden_size, output_size * pred_steps)\n",
    "\n",
    "        # Relevance markers for L1 and L2 regularizations\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "\n",
    "    ### forwards pass: How input data will be processed by the network layers\n",
    "    def forward(self, input):\n",
    "\n",
    "        # No initialisation for hidden or cell states h0, c0. Could be initialised for a given device (e.g., GPU use through .to(x.device))\n",
    "        # Inputting data (x) into the LSTM sequence and reading the output per unit and as a whole\n",
    "        lstm_output, _ = self.lstm(input)#,(h0,c0))\n",
    "        \n",
    "        # Get the last hidden state from the LSTM sequence\n",
    "        last_output = lstm_output[:, -1, :]\n",
    "        \n",
    "        # Input the last output from the LSTM into the dense linear layer, where we obtain the multi-output\n",
    "        multi_step_output = self.linear(last_output)\n",
    "        \n",
    "        # Reshape the output to get predictions for multiple future time steps\n",
    "        multi_step_output = multi_step_output.view(-1, self.pred_steps, self.output_size)\n",
    "\n",
    "        return multi_step_output\n",
    "    \n",
    "    ### Regularization functions to prevent overfitting\n",
    "    #L1 (lasso) encourages sparse weights\n",
    "    def l1_regularization_loss(self):\n",
    "        if self.training:\n",
    "            l1_loss = 0.0\n",
    "            for param in self.parameters():\n",
    "                l1_loss += torch.sum(torch.abs(param))\n",
    "            return self.l1_lambda * l1_loss\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    #L2 (Ridge) encourages small weights\n",
    "    def l2_regularization_loss(self):\n",
    "        if self.training:\n",
    "            l2_loss = 0.0\n",
    "            for param in self.parameters():\n",
    "                l2_loss += torch.sum(param ** 2)\n",
    "            return 0.5 * self.l2_lambda * l2_loss\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM training\n",
    "\n",
    "<small>Two common approaches for loss calculation in MTM problems:\n",
    "\n",
    "-__Sequence to Sequence loss__: Prediction sequence output must be the same length as the true future (target) sequence. Loss functions like Cross-Entropy Loss can be used to compare predicted and true future sequence (target).\n",
    "\n",
    "-__Element-Wise Loss__: Separate loss function between predicted sequence and true future (target) sequence for each individual time step. Final loss value is calculated as the average of all losses. Common functions MSE and MAE. We implement this one here.  <small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the loss function\n",
    "\n",
    "<small> Adding a penalty term to avoid negative predictions, as features are always positive. <small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class custom_loss(nn.Module):\n",
    "    # constructor and super from the parent nn.Module class\n",
    "    def __init__(self, penalty_weight):\n",
    "        super().__init__()\n",
    "        self.penalty_weight = penalty_weight\n",
    "        \n",
    "    def forward(self, prediction, target):\n",
    "        # mse as base loss function\n",
    "        mse_loss = nn.MSELoss()(prediction, target)\n",
    "        \n",
    "        # penalise negative prediction\n",
    "        '''\n",
    "        -prediction: negates all the values in prediction tensor\n",
    "        torch.relu: set all the now negative values (i.e., initially positive) as zero and postive (previously negative) values unchanged\n",
    "        torch.mean: average the penalty for all the previously negative predictions\n",
    "        '''\n",
    "        penalty = torch.mean(torch.relu(-prediction))\n",
    "        \n",
    "        custom_loss = mse_loss + self.penalty_weight * penalty\n",
    "        \n",
    "        return custom_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter, optimizer, loss fun, and data loader setup: DMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = X_train.shape[-1]  # Number of features in the input tensor\n",
    "hidden_size = 128  # Number of hidden units in the LSTM cell, determines how many weights will be used in the hidden state calculations\n",
    "num_layers = 1 # Number of LSTM layers per unit\n",
    "output_size = y_train.shape[-1]  # Number of output features, same as input in this case\n",
    "pred_steps = steps_out # Number of future steps to predict\n",
    "batch_size = 10 # How many windows are being processed per pass through the LSTM\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3000\n",
    "check_epochs = 100\n",
    "\n",
    "# LSTM model instance\n",
    "model = LSTM_DMS(input_size, hidden_size, output_size, pred_steps,\n",
    "                    l1_lambda=0.00, l2_lambda=0.00) \n",
    "\n",
    "# customize loss function \n",
    "penalty_weight = 10\n",
    "loss_fn = custom_loss(penalty_weight)\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) # optimizer to estimate weights and biases (backpropagation)\n",
    "                                           # requires_grad=True\n",
    "\n",
    "# Learning rate scheduler, set on min mode to decrease by factor when validation loss stops decreasing                                       \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training DMS model\n",
    "\n",
    "<sub> Early stopping feature to avoid overfitting and general training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, model_name, patience=5, verbose=False, delta=0.0001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last improvement in the monitored metric.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, print a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored metric to be considered an improvement.\n",
    "                            Default: 0.0001\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.name = model_name\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            val_loss (float): Validation loss to be monitored for improvement.\n",
    "            model (nn.Module): Model to be saved if the monitored metric improves.\n",
    "        \"\"\"\n",
    "        score = -val_loss  # Assuming lower validation loss is better.\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Saves model when validation loss decreases.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_score:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), f'{self.name}_trained_model.pt')  # Save the model's state_dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function for DMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_DMS(model, optimizer, loss_fn, loader, scheduler,\n",
    "                  num_epochs, check_epochs, \n",
    "                  X_val, y_val, saveas):\n",
    "    with open(str(saveas)+'.txt', 'w') as f:\n",
    "        print(model, file=f)\n",
    "\n",
    "        ### Early stopping feature to avoid overfitting during training, monitoring a minimum improvement threshold\n",
    "        early_stopping = EarlyStopping('DMS',patience=10, verbose=True)\n",
    "\n",
    "        for epoch in range(num_epochs): #looping through epochs\n",
    "            model.train() #set the model to train mode -- informing features to behave accordingly for training\n",
    "            \n",
    "            first_iteration = True\n",
    "            for X_batch, y_batch in loader:\n",
    "                \n",
    "                optimizer.zero_grad() # setting gradients to zero to start a new run on weight optimisation (clear accumulated from previous batch)\n",
    "\n",
    "                # Forward pass\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                # Calculate L1 and L2 regularization terms\n",
    "                l1_regularization = model.l1_regularization_loss()\n",
    "                l2_regularization = model.l2_regularization_loss()\n",
    "\n",
    "                # Add regularization terms to the loss\n",
    "                loss += l1_regularization + l2_regularization\n",
    "\n",
    "                # Backpropagation and parameter update\n",
    "                loss.backward() # calculating the gradient of the loss with respect to the model's parameters (weights and biases)\n",
    "                                # it acculmulates the gradients each time we go through the nested loop\n",
    "\n",
    "                optimizer.step() # updating parameters to minimize the loss function\n",
    "\n",
    "                # Check the shapes in the first iteration of the first epoch\n",
    "                if epoch == 0 and first_iteration:\n",
    "                    print('Input shape:', X_batch.shape)\n",
    "                    print('Output shape:', y_pred.shape)\n",
    "                    first_iteration = False\n",
    "\n",
    "            # Validation at each check epoch batch\n",
    "            if epoch % check_epochs != 0:\n",
    "                continue\n",
    "\n",
    "            model.eval() # set the model to evaluation form\n",
    "\n",
    "            with torch.no_grad(): #predictions performed with no gradient calculations\n",
    "                y_pred_train = model(X_train)\n",
    "                y_pred_val = model(X_val)\n",
    "\n",
    "                t_rmse = loss_fn(y_pred_train, y_train)\n",
    "                v_rmse = loss_fn(y_pred_val, y_val)\n",
    "\n",
    "                print('Epoch %d : train RMSE  %.4f, val RMSE %.4f ' % (epoch, t_rmse, v_rmse), file=f)\n",
    "                print('Epoch %d : train RMSE  %.4f, val RMSE %.4f ' % (epoch, t_rmse, v_rmse))\n",
    "                \n",
    "            ## Learning rate scheduler step\n",
    "            scheduler.step(v_rmse)\n",
    "\n",
    "            ## early stopping check to avoid overfitting\n",
    "            early_stopping(v_rmse, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "    ## Load the last best model before training degrades         \n",
    "    model.load_state_dict(torch.load('DMS_trained_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_DMS(model=model, optimizer=optimizer, loss_fn=loss_fn, loader=loader, scheduler=scheduler, \n",
    "             num_epochs=num_epochs, check_epochs=check_epochs,\n",
    "             X_val=X_val, y_val=y_val,\n",
    "             saveas='DMS_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub> Computing predictions with trained model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Carry out prediction with final trained model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    y_pred_train = model(X_train)\n",
    "    y_pred_val = model(X_val)\n",
    "        \n",
    "#     y_pred_test = model(X_test)\n",
    "    print(y_pred_train.shape, y_pred_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = input_size #Nd and IA\n",
    "num_cases = len(train_cases)\n",
    "colors = sns.color_palette(\"coolwarm\", num_cases)\n",
    "\n",
    "# Loop over features\n",
    "for f_idx in range(num_features):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    s_idx = -1\n",
    "    \n",
    "    for seq, case in zip(range(num_cases), train_cases):\n",
    "        # Target plots, true data from CFD\n",
    "        p = plt.plot(train[:, seq, f_idx], label=f'Target {str(case)}', color=colors[seq % len(colors)], linewidth = 2) # train has shape [times,cases,features]\n",
    "        \n",
    "        # Train predicted values\n",
    "        if seq == 0:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[:train_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Train Predicted {str(case)}',linewidth = 4)\n",
    "        else:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[train_casebatch[seq-1]:train_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Train Predicted {str(case)}', linewidth = 4)\n",
    "            \n",
    "    if f_idx == 1:\n",
    "        plt.ylim(0.6, 1.1)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim(40, 105)\n",
    "    plt.title(f'Training Prediction for {features[f_idx]}')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = input_size #Nd and IA\n",
    "num_cases = len(val_cases)\n",
    "colors = sns.color_palette(\"husl\", num_cases)\n",
    "\n",
    "# Loop over features\n",
    "for f_idx in range(num_features):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    s_idx = -1\n",
    "    \n",
    "    for seq, case in zip(range(num_cases), val_cases):\n",
    "        # Target plots, true data from CFD\n",
    "        p = plt.plot(train[:, seq, f_idx], label=f'Target {str(case)}', color=colors[seq % len(colors)],linewidth = 2) # train has shape [times,cases,features]\n",
    "        \n",
    "        # Train predicted values\n",
    "        if seq == 0:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[:val_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Val Predicted {str(case)}',linewidth = 4)\n",
    "        else:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[val_casebatch[seq-1]:val_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Val Predicted {str(case)}',linewidth = 4)\n",
    "    if f_idx == 1:\n",
    "            plt.ylim(0.6, 1.1)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim(40, 105)\n",
    "    plt.title(f'Validation prediction for {features[f_idx]}')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict future values using rollout with the LSTM model\n",
    "def rollout(model, input_seq, future_steps):\n",
    "    \n",
    "    ## setting to eval mode and dropping gradient calculation for prediction\n",
    "    model.load_state_dict(torch.load('DMS_trained_model.pt'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        reshaped_input = np.transpose(input_seq, (1,0,2)) #reshaping to case,inputdata,features\n",
    "\n",
    "        gen_seq = torch.tensor(reshaped_input).clone().detach() #detaching tensor from computational graph so gradients are not required\n",
    "\n",
    "        ### how many predicted windows will be calculated based on the input_steps\n",
    "        num_forwards = int(future_steps / steps_out) + 1\n",
    "        print(f'prediction iterates for {num_forwards} times.')\n",
    "\n",
    "        ## window predicton\n",
    "        for i in range(num_forwards):\n",
    "\n",
    "            output = model(gen_seq) #LSTM pass\n",
    "\n",
    "            gen_seq = torch.cat((torch.tensor(gen_seq), torch.tensor(output)), dim=1) # concatenates the predicted outputs to the initial input sequence\n",
    "            # This new sequence is fed as the new input tensor and used for the next set of output predictions.               \n",
    "\n",
    "    return gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = test[:steps_in,:,:]\n",
    "future_steps = test.shape[0] - steps_in\n",
    "rollout_seq = rollout(model, input_seq, future_steps)\n",
    "print(input_seq.shape)\n",
    "print(rollout_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "colors = sns.color_palette(\"cubehelix\", len(test_cases))\n",
    "\n",
    "for f_idx in range(input_size):\n",
    "    plt.figure()\n",
    "    for i, case in enumerate(test_cases):\n",
    "        r2 = r2_score(test[:,i,f_idx],rollout_seq[i,:,f_idx][:test.shape[0]])\n",
    "\n",
    "        p = plt.plot(test[:,i,f_idx], label=f'Target {case}, r2:{r2:.4f}',color = colors[i % len(colors)],linewidth = 2)\n",
    "        plt.plot(rollout_seq[i,:,f_idx],c=p[0].get_color(), linestyle=':',label=r'Prediction',linewidth = 4)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'Prediction for: {features[f_idx]}')\n",
    "    plt.xlim(35, 105)\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Sequence-to-Sequence S2S approach\n",
    "<small> - __Sequence-to-Sequence__: Use the LSTM's output for each time step and feed it back as input for the subsequent time step. In this way, LSTM generates predictions step-by-step for each future time step.\n",
    "\n",
    "This approach consists of two LSTMs, an encoder and a decoder:\n",
    "\n",
    "- The **encoder LSTM** processes the input sequence an generates an internal (encoded) representation (hidden state), summarizing the input information.\n",
    "- The **decoder LSTM** uses the encoded state (final hidden state) as its initial hidden state and then generates an output sequence step by step.\n",
    "\n",
    "We consider the same windowing data-processing step used prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM ENCODER/DECODER AND PREDICTION CLASSES\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_encoder(nn.Module):\n",
    "\n",
    "    # Same as LSTM DMS constructor but with no pred_steps or linear layer as encoder feeds decoder LSTM through the hidden states\n",
    "    def __init__(self,input_size,hidden_size, num_layers=1):\n",
    "        super(LSTM_encoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "    # Take the input sequences and output the hidden states for the LSTM decoder section\n",
    "    def forward(self, encoder_input):\n",
    "        ''' \n",
    "        return encoder_hidden_states: outputs the last time hidden and cell state to be fed into the LSTM decoder\n",
    "        \n",
    "        input shape: (batch_size, input steps/input window, input_size=num_features)\n",
    "        output shape: (input_size=num_features, hidden_size)\n",
    "        '''\n",
    "        _, (h_n_encoder,c_n_encoder) = self.lstm(encoder_input) #ignoring output (hidden states) for all times and only saving a tuple with the last timestep cell and hidden state\n",
    "        \n",
    "        return (h_n_encoder,c_n_encoder)\n",
    "\n",
    "class LSTM_decoder(nn.Module):\n",
    "\n",
    "    ## Same constructor as DMS as now we are decoding the final LSTM cell through a linear layer to generate the final output \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTM_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_states):\n",
    "        '''\n",
    "        return \n",
    "        lstm_output: returns decoded hidden states as output for all times \n",
    "        \n",
    "        input shape: (batch_size, 1, input_size=num_features) the last time step\n",
    "        output shape: (batch_size, input_size=num_features)\n",
    "        '''\n",
    "\n",
    "        # LSTM cell is initialized with the encoder cell and hidden states\n",
    "                # Input tensor is unsqueezed to introduce an additional dimension in axis = 1 to perform LSTM calculations normally for 1 step\n",
    "        lstm_output, _ = self.lstm(decoder_input.unsqueeze(1), encoder_states) #Similar to DMS, output is saved, representing all hidden states per timestep\n",
    "        \n",
    "        ## output tensor is squeezed, removing the aritificial time dimension in axis = 1, as it will be looped during prediction for each time and appended to a 3D tensor.\n",
    "        output = self.linear(lstm_output.squeeze(1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class LSTM_S2S(nn.Module):\n",
    "    ''' Double LSTM Encoder-decoder architecture to make predictions '''\n",
    "\n",
    "    #Constructing the encoder decoder LSTM architecture\n",
    "    def __init__(self, input_size, hidden_size, output_size, pred_steps):\n",
    "        super(LSTM_S2S,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.pred_steps = pred_steps #steps out = output window\n",
    "        \n",
    "        self.encoder = LSTM_encoder(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.decoder = LSTM_decoder(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "    def forward(self,input_tensor):\n",
    "        '''\n",
    "        input_tensor: shape (batch_size, input steps = input window, input_size=num_features)\n",
    "        pred_steps: number of time steps to predict\n",
    "        return np_outputs: array containing predictions\n",
    "        '''\n",
    "                \n",
    "        # encode input_tensor\n",
    "        encoder_states = self.encoder(input_tensor)\n",
    "\n",
    "        # initialize output tensor for prediction\n",
    "        outputs = torch.zeros(input_tensor.shape[0], self.pred_steps, input_tensor.shape[2]) #shape = batch_size, steps_out, num_features\n",
    "\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[:,-1,:] # Taking last value in the window/sequence\n",
    "        decoder_input_states = encoder_states\n",
    "\n",
    "        # predictions carried out on the decoder for each time in the output window = steps_out\n",
    "        for t in range(self.pred_steps):\n",
    "            decoder_output = self.decoder(decoder_input,decoder_input_states)\n",
    "            outputs[:,t,:] = decoder_output\n",
    "            # prediction done recursively\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "        np_outputs = outputs.detach().numpy() ## detaching from gradient requirements during prediction\n",
    "\n",
    "        return torch.from_numpy(np_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Training: S2S\n",
    "\n",
    "<sub> We keep using the same early stopping and custom loss functions defined for DMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter, optimizer, loss fun, and data loader setup: S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = X_train.shape[-1]  # Number of features in the input tensor\n",
    "hidden_size = 128  # Number of hidden units in the LSTM cell, determines how many weights will be used in the hidden state calculations\n",
    "num_layers = 1 # Number of LSTM layers per unit\n",
    "output_size = y_train.shape[-1]  # Number of output features, same as input in this case\n",
    "pred_steps = steps_out # Number of future steps to predict\n",
    "batch_size = 10 # How many windows are being processed per pass through the LSTM\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3000\n",
    "check_epochs = 100\n",
    "\n",
    "# For teacher forcing\n",
    "tf_ratio = 0.1 \n",
    "dynamic_tf = False\n",
    "\n",
    "# Pick one type of model #\n",
    "model = LSTM_S2S(input_size, hidden_size, output_size, pred_steps) # create out NN\n",
    "\n",
    "\n",
    "# customize loss function \n",
    "penalty_weight = 10\n",
    "loss_fn = custom_loss(penalty_weight)\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate) # optimizer to estimate weights and biases (backpropagation)\n",
    "                                           # requires_grad=True\n",
    "\n",
    "# Learning rate scheduler, set on min mode to decrease by factor when validation loss stops decreasing                                       \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub> Encoding-decoding LSTM architectures are not coded here, as they have been defined already in the S2S class. Hence, the training here only calls the forward functions from each step as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_S2S(model, optimizer, loss_fn, loader,scheduler, \n",
    "                 num_epochs, check_epochs, pred_steps,\n",
    "                 training_prediction, tf_ratio, dynamic_tf,\n",
    "                 X_val, y_val,\n",
    "                 saveas):\n",
    "    ''' \n",
    "    training_prediction: ('recursive'/'teacher_forcing'/'mixed')\n",
    "    tf_ratio: float[0,1] \n",
    "                relevance on teacher forcing when training_prediction = 'teacher_forcing'.\n",
    "                For each batch, a random number is generated. \n",
    "                If the number is less than tf_ratio, tf is used; otherwise, prediction is done recursively.\n",
    "                If tf_ratio = 1, only tf is used.\n",
    "    dynamic_tf: (True/False)\n",
    "                dynamic teacher forcing reduces the amount of teacher forcing for each epoch\n",
    "    \n",
    "    return loss: array of loss function for each epoch\n",
    "    '''\n",
    "\n",
    "    # save the training model\n",
    "    with open(str(saveas)+'.txt', 'w') as f:\n",
    "        print(model, file=f)\n",
    "\n",
    "        ### Early stopping feature to avoid overfitting during training, monitoring a minimum improvement threshold\n",
    "        early_stop = EarlyStopping('S2S',patience=10, verbose=True)\n",
    "\n",
    "        for epoch in range(num_epochs): #looping through training epochs\n",
    "            \n",
    "            model.train() #setting model to training function to deactivate regularization and other training features\n",
    "            first_iteration = True\n",
    "\n",
    "            for X_batch, y_batch in loader:\n",
    "\n",
    "                # initializing output tensor\n",
    "                outputs = torch.zeros(X_batch.shape[0], pred_steps, X_batch.shape[2]) #shape = (batch_size,steps_out,num_features)\n",
    "\n",
    "                #reset gradients from previous training step\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                #going through the LSTM encoder layer: return hidden and cell states\n",
    "                encoder_states = model.encoder(X_batch)\n",
    "\n",
    "                # decoder starting with teacher forcing: input set as last timestep from input batch\n",
    "                decoder_input = X_batch[:,-1,:] # in shape of (batch_size, input_size = num_features)\n",
    "                decoder_input_states = encoder_states\n",
    "\n",
    "                #Considering variations in training methods per batch\n",
    "                if training_prediction == 'recursive':\n",
    "                        \n",
    "                    # recursive prediction: predicted output is fed\n",
    "                        for t in range(pred_steps):\n",
    "                            decoder_output = model.decoder(decoder_input, decoder_input_states)\n",
    "                            outputs[:,t,:] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "\n",
    "                if training_prediction == 'teacher_forcing':\n",
    "                        \n",
    "                    # predict using teacher forcing: target is fed\n",
    "                        if random.random() < tf_ratio:\n",
    "                            for t in range(pred_steps):\n",
    "                                decoder_output = model.decoder(decoder_input, decoder_input_states)\n",
    "                                outputs[:,t,:] = decoder_output\n",
    "                                decoder_input = y_batch[:,t,:] # target fed from y_batch in shape of (batch_size, input_size = num_features)\n",
    "                        # predict recursively\n",
    "                        else:\n",
    "                            for t in range(pred_steps):\n",
    "                                decoder_output = model.decoder(decoder_input, decoder_input_states)\n",
    "                                outputs[:,t,:] = decoder_output\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "\n",
    "                if training_prediction == 'mixed':\n",
    "\n",
    "                    # both types of training methods used in the same batch, alternating stochastically based on tf_ratio\n",
    "                    for t in range(pred_steps):\n",
    "                        decoder_output = model.decoder(decoder_input, decoder_input_states)\n",
    "                        outputs[:,t,:] = decoder_output\n",
    "\n",
    "                        ## Teaching method chosen per timestep within the given batch\n",
    "                        # teacher forcing\n",
    "                        if random.random() < tf_ratio:\n",
    "                            decoder_input = y_batch[:,t,:]\n",
    "                        # recursive:\n",
    "                        else:\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "                loss = loss_fn(outputs,y_batch)\n",
    "\n",
    "                # Backpropagation and parameter update\n",
    "                loss.backward() # calculating the gradient of the loss with respect to the model's parameters (weights and biases)\n",
    "                                # it acculmulates the gradients each time we go through the nested loop\n",
    "\n",
    "                optimizer.step() # updating parameters to minimize the loss function\n",
    "\n",
    "                # Check the shapes in the first iteration of the first epoch\n",
    "                if epoch == 0 and first_iteration:\n",
    "                    print('Input shape:', X_batch.shape)\n",
    "                    print('Output shape:', outputs.shape)\n",
    "                    first_iteration = False\n",
    "                \n",
    "            # dynamic teacher forcing\n",
    "            if dynamic_tf and tf_ratio > 0:\n",
    "                tf_ratio = tf_ratio - 0.02 ## if dynamic tf active, the amount of teacher forcing is reduced per epoch\n",
    "\n",
    "                # Validation at each check epoch batch\n",
    "            if epoch % check_epochs != 0:\n",
    "                continue\n",
    "\n",
    "            model.eval() # set the model to evaluation form, disabling regularisation and training features\n",
    "\n",
    "            with torch.no_grad(): #predictions performed with no gradient calculations\n",
    "                y_pred_train = model(X_train)\n",
    "                y_pred_val = model(X_val)\n",
    "\n",
    "                t_rmse = loss_fn(y_pred_train, y_train)\n",
    "                v_rmse = loss_fn(y_pred_val, y_val)\n",
    "\n",
    "                print('Epoch %d : train RMSE  %.4f, val RMSE %.4f ' % (epoch, t_rmse, v_rmse), file=f)\n",
    "                print('Epoch %d : train RMSE  %.4f, val RMSE %.4f ' % (epoch, t_rmse, v_rmse))\n",
    "                \n",
    "            ## Learning rate scheduler step\n",
    "            scheduler.step(v_rmse)\n",
    "\n",
    "            ## early stopping check to avoid overfitting\n",
    "            early_stop(v_rmse, model)\n",
    "\n",
    "            if early_stop.early_stop:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "    ## Load the last best model before training degrades         \n",
    "    model.load_state_dict(torch.load('S2S_trained_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_S2S(model=model, optimizer=optimizer, loss_fn=loss_fn, loader=loader, scheduler=scheduler,\n",
    "            num_epochs=num_epochs, check_epochs=check_epochs, pred_steps=pred_steps,\n",
    "            training_prediction= 'mixed',tf_ratio=tf_ratio,dynamic_tf=dynamic_tf,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            saveas='S2S_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load final model back for train, validation visualization and later test prediction\n",
    "\n",
    "# S2S instance\n",
    "model = LSTM_S2S(input_size, hidden_size, output_size, pred_steps)\n",
    "\n",
    "# load the final model\n",
    "model.load_state_dict(torch.load('S2S_trained_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation data sets visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try:\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "    except:\n",
    "        y_pred_train = model(X_train)\n",
    "        y_pred_val = model(X_val)\n",
    "        \n",
    "#     y_pred_test = model(X_test)\n",
    "    print(y_pred_train.shape, y_pred_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = input_size #Nd and IA\n",
    "num_cases = len(train_cases)\n",
    "colors = sns.color_palette(\"coolwarm\", num_cases)\n",
    "\n",
    "# Loop over features\n",
    "for f_idx in range(num_features):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    s_idx = -1\n",
    "    \n",
    "    for seq, case in zip(range(num_cases), train_cases):\n",
    "        # Target plots, true data from CFD\n",
    "        p = plt.plot(train[:, seq, f_idx], label=f'Target {str(case)}', color=colors[seq % len(colors)], linewidth = 2) # train has shape [times,cases,features]\n",
    "        \n",
    "        # Train predicted values\n",
    "        if seq == 0:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[:train_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Train Predicted {str(case)}',linewidth = 4)\n",
    "        else:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[train_casebatch[seq-1]:train_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Train Predicted {str(case)}', linewidth = 4)\n",
    "            \n",
    "    if f_idx == 1:\n",
    "        plt.ylim(0.6, 1.1)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim(40, 105)\n",
    "    plt.title(f'Training Prediction for {features[f_idx]}')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = input_size #Nd and IA\n",
    "num_cases = len(val_cases)\n",
    "colors = sns.color_palette(\"husl\", num_cases)\n",
    "\n",
    "# Loop over features\n",
    "for f_idx in range(num_features):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    s_idx = -1\n",
    "    \n",
    "    for seq, case in zip(range(num_cases), val_cases):\n",
    "        # Target plots, true data from CFD\n",
    "        p = plt.plot(train[:, seq, f_idx], label=f'Target {str(case)}', color=colors[seq % len(colors)],linewidth = 2) # train has shape [times,cases,features]\n",
    "        \n",
    "        # Train predicted values\n",
    "        if seq == 0:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[:val_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Val Predicted {str(case)}',linewidth = 4)\n",
    "        else:\n",
    "            plt.plot(range(wid_size-1,len(train)),\n",
    "                     y_pred_train[val_casebatch[seq-1]:val_casebatch[seq],s_idx,f_idx], \n",
    "                     c=p[0].get_color(),linestyle=':', label=f'Val Predicted {str(case)}',linewidth = 4)\n",
    "    if f_idx == 1:\n",
    "            plt.ylim(0.6, 1.1)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim(40, 105)\n",
    "    plt.title(f'Validation prediction for {features[f_idx]}')\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout function to carry out predictions in test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict future values using rollout with the LSTM model\n",
    "def rollout(model, input_seq, future_steps):\n",
    "    \n",
    "    ## setting to eval mode and dropping gradient calculation for prediction\n",
    "    model.load_state_dict(torch.load('S2S_trained_model.pt'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        reshaped_input = np.transpose(input_seq, (1,0,2)) #reshaping to case,inputdata,features\n",
    "\n",
    "        gen_seq = torch.tensor(reshaped_input).clone().detach() #detaching tensor from computational graph so gradients are not required\n",
    "\n",
    "        ### how many predicted windows will be calculated based on the input_steps\n",
    "        num_forwards = int(future_steps / steps_out) + 1\n",
    "        print(f'prediction iterates for {num_forwards} times.')\n",
    "\n",
    "        ## window predicton\n",
    "        for i in range(num_forwards):\n",
    "\n",
    "            output = model(gen_seq) #LSTM pass\n",
    "\n",
    "            gen_seq = torch.cat((torch.tensor(gen_seq), torch.tensor(output)), dim=1) # concatenates the predicted outputs to the initial input sequence\n",
    "            # This new sequence is fed as the new input tensor and used for the next set of output predictions.               \n",
    "\n",
    "    return gen_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = test[:steps_in,:,:]\n",
    "future_steps = test.shape[0] - steps_in\n",
    "rollout_seq = rollout(model, input_seq, future_steps)\n",
    "print(input_seq.shape)\n",
    "print(rollout_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "colors = sns.color_palette(\"cubehelix\", len(test_cases))\n",
    "\n",
    "for f_idx in range(input_size):\n",
    "    plt.figure()\n",
    "    for i, case in enumerate(test_cases):\n",
    "        r2 = r2_score(test[:,i,f_idx],rollout_seq[i,:,f_idx][:test.shape[0]])\n",
    "\n",
    "        p = plt.plot(test[:,i,f_idx], label=f'Target {case}, r2:{r2:.4f}',color = colors[i % len(colors)],linewidth = 2)\n",
    "        plt.plot(rollout_seq[i,:,f_idx],c=p[0].get_color(), linestyle=':',label=r'Prediction',linewidth = 4)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'Prediction for: {features[f_idx]}')\n",
    "    plt.xlim(35, 105)\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel(f'Scaled {features[f_idx]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
